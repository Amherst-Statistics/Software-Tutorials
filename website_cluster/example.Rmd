---
title: "Example simulation"
author: 'Nicholas Horton (nhorton@amherst.edu) and Shukry Zablah'
date: 'June 21, 2018'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
library(mosaic)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(xtable); options(xtable.comment = FALSE)
set.seed(100)
```

Conduct a simulation study (with numsim at least 500) with n=25 and n=100 observations. Generate data from a true model where Y = 0.5 - 1 * X where the residuals are normal with mean zero but the standard deviation depends on the X’s. Let the distribution of the X’s be uniform [0, 2] and the standard deviation of the residuals be equal to $0.1+X^2$. Repeat the process for a scenario where your residuals are normally distributed with equal variance = 1 for all X’s. Compare the coverage results and shape of the distribution of the slope parameter.

#### Solution
```{r, include = FALSE}
plot_slopes <- function(df, title){
  p <- ggplot(df, aes(x=slope)) + geom_density() + theme_fivethirtyeight() + geom_vline( xintercept =-1, col = 'red') + labs(x = 'slopes', title = title) + scale_x_continuous(limits=c(-5,3)) + scale_y_continuous(limits=c(0,2.5))
  return(p)
}
```

First we generate the data, fit the model, and extract the slope for the unequal variance problem

```{r}
numsim <- 2000
gen_sim <- function(n) {
  x <- runif(n, min = 0, max = 2)
  errors <- rnorm(n, 0, sd = 1 + x^2) # variance is not equal
  y <- 0.5 - x + errors
  return(data.frame(y, x))
}
fit_slope <- function(n) {
  df <- gen_sim(n)
  mod <- lm(y ~ x, df)
  slope <- coef(mod)['x']
  conf <- stats::confint(mod)
  covered <- (-1 > conf['x', 1]) && (-1 < conf['x', 2])
  return(data.frame(slope, covered))
}

```

Next we do the same assuming equal variances.

```{r}
gen_sim_equal <- function(n) {
  x <- runif(n, min = 0, max = 2)
  errors <- rnorm(n, 0, sd = 1) # variance is equal
  y <- 0.5 - x + errors
  return(data.frame(y,x))
}
fit_slope_equal <- function(n) {
  df <- gen_sim_equal(n)
  mod <- lm(y ~ x, df)
  slope <- coef(mod)['x']
  conf <- confint(mod)
  covered <- (-1 > conf['x', 1]) && (-1 < conf['x', 2])
  return(data.frame(slope, covered))
}
```

## Plots

```{r echo=FALSE}
slopes_25 <- do(numsim) * fit_slope(25)
slopes_equal_25 <- do(numsim) * fit_slope_equal(25)
slopes_100 <- do(numsim) * fit_slope(100)
slopes_equal_100 <- do(numsim) * fit_slope_equal(100)
```

```{r, fig.height=3, echo=FALSE, warning=F, eval=F, include=F}
p1 <- plot_slopes(slopes_25, "Unequal Var, n = 25")
p2 <- plot_slopes(slopes_equal_25, "Equal Var, n = 25")
p3 <- plot_slopes(slopes_100, "Unequal Var, n = 100")
p4 <- plot_slopes(slopes_equal_100, "Equal Var, n = 100")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

```{r, fig.height=3, echo = FALSE}
slopes_25$label <- 'Unequal Var, n = 25'
slopes_equal_25$label <- 'Equal Var, n = 25'
slopes_100$label <- 'Unequal Var, n = 100'
slopes_equal_100$label <- 'Equal Var, n = 100'
complete <- rbind(slopes_25, slopes_100, slopes_equal_100, slopes_equal_25)
ggplot(complete, aes(x = slope, color = label)) + geom_density() + theme_fivethirtyeight() + labs(title = 'Distribution of Estimated Slope')
```

## Coverage Rate and Confidence Interval
```{r}
sims <- list(slopes_equal_25, slopes_25, slopes_equal_100, slopes_100)
df <- data.frame('coverage' = numeric(), 'lower' = numeric(), 
                 'upper' = numeric())
for (sim in sims){
  b <- binom.test(x = sum(sim$covered), n = length(sim$covered))
  rate <- sum(sim$covered)/length(sim$covered)
  lower <- b$conf.int[1]
  upper <- b$conf.int[2]
  df <- rbind(df, data.frame('coverage' = rate, 'lower' = lower, 
                             'upper ' = upper))
}
```
```{r, results="asis"}
row.names(df) <- c('Equal Var, n = 25', 'Unequal Var, n = 25',
                  'Equal Var, n = 100', 'Unequal Var, n = 100')
xtable(df)
```

## Conclusion

The key idea to this problem was to determine why/if we should care about the equal variance assumption for linear regression. We have been warned about heterscedasticity, as noted in Rice p. 554. Rice says that if the error variance is not constant, standard errors and confidence intervals may be misleading. We tested whether that is true or not for sample sizes n=25 and n=100.

When looking at the distribution for the slope parameters for all four cases (n=25 or n=100, and equal variance or not equal variance), they look relatively similar and normal, centered around -1. However, we do see that for both n=25 and n=100, the distributions for the unequal variance (blue and purple lines on the graph) are wider than when the variance is equal.

The coverage rates are quite different for the different cases. As shown in the chart above, when n=25, the coverage rate for unequal variance is much lower than the equal variance (0.91 compared to 0.94). Also, the confidence interval for the unequal variance case is larger than the equal variance case (a range of 0.03 compared to a range of 0.02). This indicates that when there is unequal variance, not only is there less coverage, but also less consistency in coverage rates as well. The same conclusions were drawn when n=100, as you can see from the chart.

We determined that the equal variance assumption is important, and especially when you have small sample sizes of 25 and 100. In summary, the confidence interval for unequal variances is wider than equal variances when n=25 and when n=100, so the method undercovers (*anti-conservative*: bad!).
